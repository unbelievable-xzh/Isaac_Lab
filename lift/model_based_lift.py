"""Minimal model-based reinforcement learning demo for the lift task.

This example shows how to collect a dataset from the Isaac Lab lift environment,
fit a simple neural network dynamics model, and use it for action selection via
random shooting model predictive control (MPC).

The implementation is intentionally compact: it focuses on clarity and keeps
all logic in a single file so that it is easy to adapt for experimentation.

Usage
-----
Run the module directly from the repository root. The script will (1) gather a
random dataset from the Lift environment, (2) train the learned dynamics model,
and (3) evaluate the MPC controller:

.. code-block:: bash

    python -m lift.model_based_lift --headless

Commonly-adjusted arguments include ``--dataset-size`` (number of transitions),
``--epochs`` (model training epochs), and ``--mpc-horizon`` (planning depth).

The script works out-of-the-box; no other project files need modification.
Use ``python -m lift.model_based_lift --show-usage`` to print the same
instructions at the command line.

For end-to-end training with Isaac Lab's official demos, this module also
exposes :func:`create_model_based_hybrid_env`. The factory registers the
``Isaac-Lift-Cube-Franka-Hybrid-v0`` Gym environment, which wraps the official
Franka lift task and blends policy actions with MPC suggestions generated by the
learned model. Launch the standard training script with that environment ID to
obtain a model-free learner augmented by the model-based controller.
"""

from __future__ import annotations

import argparse
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Dict, List, MutableMapping, Optional, Sequence

import gymnasium as gym
from gymnasium.spaces import Box
import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset


# -----------------------------------------------------------------------------
# Data utilities
# -----------------------------------------------------------------------------


@dataclass
class Transition:
    """Container for a single environment transition."""

    obs: np.ndarray
    action: np.ndarray
    reward: float
    next_obs: np.ndarray
    done: bool


class TransitionDataset(Dataset):
    """PyTorch dataset for environment transitions."""

    def __init__(self, transitions: Sequence[Transition]):
        self._observations = torch.as_tensor(np.stack([t.obs for t in transitions]), dtype=torch.float32)
        self._actions = torch.as_tensor(np.stack([t.action for t in transitions]), dtype=torch.float32)
        self._rewards = torch.as_tensor([t.reward for t in transitions], dtype=torch.float32).unsqueeze(-1)
        self._next_observations = torch.as_tensor(
            np.stack([t.next_obs for t in transitions]), dtype=torch.float32
        )

    def __len__(self) -> int:  # pragma: no cover - trivial
        return self._observations.shape[0]

    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        return (
            self._observations[index],
            self._actions[index],
            self._rewards[index],
            self._next_observations[index],
        )


def collect_transitions(
    env: gym.Env,
    num_samples: int,
    max_episode_steps: int,
) -> List[Transition]:
    """Collect transitions using a random policy."""

    transitions: List[Transition] = []
    obs, _ = env.reset()
    steps_remaining = max_episode_steps

    while len(transitions) < num_samples:
        action = env.action_space.sample()
        next_obs, reward, terminated, truncated, _ = env.step(action)
        transitions.append(Transition(obs=obs, action=action, reward=reward, next_obs=next_obs, done=terminated))

        steps_remaining -= 1
        done = terminated or truncated or steps_remaining <= 0
        if done:
            obs, _ = env.reset()
            steps_remaining = max_episode_steps
        else:
            obs = next_obs

    return transitions


# -----------------------------------------------------------------------------
# Dynamics model
# -----------------------------------------------------------------------------


class DynamicsModel(nn.Module):
    """Simple multi-layer perceptron dynamics model."""

    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int = 256):
        super().__init__()
        self.obs_dim = obs_dim
        self.net = nn.Sequential(
            nn.Linear(obs_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, obs_dim + 1),
        )

    def forward(self, obs: torch.Tensor, action: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        x = torch.cat([obs, action], dim=-1)
        output = self.net(x)
        delta_obs = output[..., : self.obs_dim]
        reward = output[..., self.obs_dim : self.obs_dim + 1]
        return obs + delta_obs, reward


def train_dynamics_model(
    model: DynamicsModel,
    dataset: Dataset,
    batch_size: int,
    epochs: int,
    device: torch.device,
) -> None:
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    loss_fn = nn.MSELoss()

    model.train()
    for _ in range(epochs):
        for obs, action, reward, next_obs in dataloader:
            obs = obs.to(device)
            action = action.to(device)
            reward = reward.to(device)
            next_obs = next_obs.to(device)

            pred_next_obs, pred_reward = model(obs, action)
            loss = loss_fn(pred_next_obs, next_obs) + loss_fn(pred_reward, reward)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()


# -----------------------------------------------------------------------------
# Model predictive control
# -----------------------------------------------------------------------------


@dataclass
class MPCConfig:
    horizon: int = 5
    num_action_sequences: int = 256


def plan_action(
    model: DynamicsModel,
    obs: np.ndarray,
    action_space: gym.spaces.Box,
    config: MPCConfig,
    device: torch.device,
) -> np.ndarray:
    """Random shooting MPC planner."""

    obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)

    low = torch.as_tensor(action_space.low, dtype=torch.float32, device=device)
    high = torch.as_tensor(action_space.high, dtype=torch.float32, device=device)
    action_dim = low.shape[0]

    best_return = -float("inf")
    best_first_action = None

    model.eval()
    with torch.no_grad():
        for _ in range(config.num_action_sequences):
            actions = torch.rand((config.horizon, action_dim), device=device) * (high - low) + low
            rollout_obs = obs_t.clone()
            total_reward = torch.zeros(1, device=device)

            for action in actions:
                rollout_obs, reward = model(rollout_obs, action.unsqueeze(0))
                total_reward += reward.squeeze(0)

            rollout_return = float(total_reward.item())
            if rollout_return > best_return:
                best_return = rollout_return
                best_first_action = actions[0]

    assert best_first_action is not None  # safety
    return best_first_action.cpu().numpy()


# -----------------------------------------------------------------------------
# Planner abstraction and hybrid environment wrapper
# -----------------------------------------------------------------------------


class ModelBasedPlanner:
    """Convenience wrapper around a trained dynamics model and MPC planner."""

    def __init__(
        self,
        model: DynamicsModel,
        action_space: Box,
        mpc_cfg: MPCConfig,
        device: torch.device,
    ) -> None:
        self.model = model
        self.action_space = action_space
        self.mpc_cfg = mpc_cfg
        self.device = device

    @property
    def obs_dim(self) -> int:
        return self.model.obs_dim

    @property
    def action_dim(self) -> int:
        return int(self.action_space.shape[0])

    def plan(self, observation: np.ndarray) -> np.ndarray:
        """Plan a single action for the provided observation."""

        return plan_action(self.model, observation, self.action_space, self.mpc_cfg, self.device)

    def plan_batch(self, observations: np.ndarray) -> np.ndarray:
        """Plan actions for a batch of observations."""

        obs_array = np.asarray(observations, dtype=np.float32)
        if obs_array.ndim == 1:
            return self.plan(obs_array)
        actions = [self.plan(obs) for obs in obs_array]
        return np.stack(actions, axis=0)

    def save(self, path: Path) -> None:
        """Persist the planner for reuse."""

        path.parent.mkdir(parents=True, exist_ok=True)
        payload = {
            "state_dict": self.model.state_dict(),
            "obs_dim": self.obs_dim,
            "action_dim": self.action_dim,
            "action_low": self.action_space.low.tolist(),
            "action_high": self.action_space.high.tolist(),
            "mpc_cfg": asdict(self.mpc_cfg),
        }
        torch.save(payload, path)

    @classmethod
    def load(cls, path: Path, device: torch.device) -> "ModelBasedPlanner":
        """Load a previously-saved planner."""

        payload = torch.load(path, map_location=device)
        model = DynamicsModel(obs_dim=payload["obs_dim"], action_dim=payload["action_dim"])
        model.load_state_dict(payload["state_dict"])
        model.to(device)
        action_space = Box(
            low=np.asarray(payload["action_low"], dtype=np.float32),
            high=np.asarray(payload["action_high"], dtype=np.float32),
        )
        mpc_cfg = MPCConfig(**payload["mpc_cfg"])
        return cls(model=model, action_space=action_space, mpc_cfg=mpc_cfg, device=device)

    @classmethod
    def train_from_random_rollouts(
        cls,
        env: gym.Env,
        dataset_size: int,
        max_episode_steps: int,
        epochs: int,
        batch_size: int,
        mpc_cfg: MPCConfig,
        device: torch.device,
    ) -> "ModelBasedPlanner":
        """Collect rollouts from ``env`` and fit the dynamics model."""

        transitions = collect_transitions(env, dataset_size, max_episode_steps)
        dataset = TransitionDataset(transitions)
        obs_dim = dataset[0][0].shape[0]
        action_dim = dataset[0][1].shape[0]
        model = DynamicsModel(obs_dim=obs_dim, action_dim=action_dim).to(device)
        train_dynamics_model(model, dataset, batch_size, epochs, device)
        return cls(model=model, action_space=env.action_space, mpc_cfg=mpc_cfg, device=device)


_PLANNER_CACHE: Dict[str, ModelBasedPlanner] = {}


def _planner_cache_key(base_env_id: str, planner_cfg: MutableMapping[str, object]) -> str:
    """Create a stable cache key for planner reuse."""

    serializable_items = tuple(sorted((key, str(value)) for key, value in planner_cfg.items()))
    return f"{base_env_id}|{serializable_items}"


class ModelBasedHybridWrapper(gym.Wrapper):
    """Blend model-free actions with MPC suggestions during training."""

    def __init__(
        self,
        env: gym.Env,
        planner: ModelBasedPlanner,
        *,
        initial_blend: float = 0.5,
        min_blend: float = 0.0,
        blend_decay_steps: Optional[int] = None,
    ) -> None:
        super().__init__(env)
        self._planner = planner
        self._initial_blend = float(np.clip(initial_blend, 0.0, 1.0))
        self._min_blend = float(np.clip(min_blend, 0.0, 1.0))
        self._blend_decay_steps = blend_decay_steps if blend_decay_steps and blend_decay_steps > 0 else None
        self._step_count = 0
        self._last_obs: Optional[np.ndarray] = None

    def reset(self, **kwargs):  # type: ignore[override]
        obs, info = self.env.reset(**kwargs)
        self._step_count = 0
        self._last_obs = np.asarray(obs, dtype=np.float32)
        return obs, info

    def _current_blend(self) -> float:
        if self._blend_decay_steps is None:
            return self._initial_blend
        progress = min(1.0, self._step_count / self._blend_decay_steps)
        return (1.0 - progress) * self._initial_blend + progress * self._min_blend

    def step(self, action):  # type: ignore[override]
        if self._last_obs is None:
            raise RuntimeError("Environment must be reset before calling step().")

        teacher_action = self._planner.plan_batch(self._last_obs)
        action_np = np.asarray(action, dtype=np.float32)
        teacher_np = np.asarray(teacher_action, dtype=np.float32)

        if teacher_np.shape != action_np.shape:
            if teacher_np.ndim == action_np.ndim - 1:
                teacher_np = np.broadcast_to(teacher_np, action_np.shape)
            else:
                raise ValueError(
                    "Model-based action shape mismatch: received"
                    f" {teacher_np.shape} for policy action shape {action_np.shape}."
                )

        blend = self._current_blend()
        combined = (1.0 - blend) * action_np + blend * teacher_np
        combined = np.clip(combined, self.env.action_space.low, self.env.action_space.high)

        obs, reward, terminated, truncated, info = self.env.step(combined)
        self._step_count += 1
        self._last_obs = np.asarray(obs, dtype=np.float32)

        info = dict(info)
        info["model_based_action"] = teacher_np
        info["combined_action"] = combined
        info["model_based_blend"] = blend
        return obs, reward, terminated, truncated, info


def create_model_based_hybrid_env(
    *,
    base_env_id: str = "Isaac-Lift-Cube-Franka-v0",
    model_based_cfg: Optional[MutableMapping[str, object]] = None,
    **base_env_kwargs,
) -> gym.Env:
    """Instantiate a lift environment that mixes model-free and model-based control."""

    default_model_path = Path.home() / ".cache" / "isaac_lab" / "lift_model_based_teacher.pt"
    cfg: Dict[str, object] = {
        "dataset_size": 2000,
        "max_episode_steps": 200,
        "epochs": 25,
        "batch_size": 128,
        "mpc_horizon": 5,
        "mpc_samples": 256,
        "initial_blend": 0.5,
        "min_blend": 0.0,
        "blend_decay_steps": None,
        "collect_headless": True,
        "model_path": default_model_path,
    }
    if model_based_cfg is not None:
        cfg.update(model_based_cfg)

    planner_cfg: Dict[str, object] = {
        "dataset_size": int(cfg["dataset_size"]),
        "max_episode_steps": int(cfg["max_episode_steps"]),
        "epochs": int(cfg["epochs"]),
        "batch_size": int(cfg["batch_size"]),
        "mpc_horizon": int(cfg["mpc_horizon"]),
        "mpc_samples": int(cfg["mpc_samples"]),
    }
    model_path_value = cfg.get("model_path")
    checkpoint_path: Optional[Path] = None
    if model_path_value:
        checkpoint_path = Path(model_path_value).expanduser().resolve()
        planner_cfg["model_path"] = str(checkpoint_path)

    cache_key = _planner_cache_key(base_env_id, planner_cfg)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    planner = _PLANNER_CACHE.get(cache_key)
    if planner is None:
        if checkpoint_path and checkpoint_path.exists():
            planner = ModelBasedPlanner.load(checkpoint_path, device)
        else:
            collect_kwargs = dict(base_env_kwargs)
            if cfg.get("collect_headless", True):
                collect_kwargs["headless"] = True
            data_env = gym.make(base_env_id, **collect_kwargs)
            try:
                dataset_size = int(planner_cfg["dataset_size"])
                max_episode_steps = int(planner_cfg["max_episode_steps"])
                epochs = int(planner_cfg["epochs"])
                batch_size = int(planner_cfg["batch_size"])
                horizon = int(planner_cfg["mpc_horizon"])
                samples = int(planner_cfg["mpc_samples"])
                planner = ModelBasedPlanner.train_from_random_rollouts(
                    data_env,
                    dataset_size=dataset_size,
                    max_episode_steps=max_episode_steps,
                    epochs=epochs,
                    batch_size=batch_size,
                    mpc_cfg=MPCConfig(horizon=horizon, num_action_sequences=samples),
                    device=device,
                )
            finally:
                data_env.close()
            if checkpoint_path:
                planner.save(checkpoint_path)
        _PLANNER_CACHE[cache_key] = planner

    env = gym.make(base_env_id, **base_env_kwargs)
    decay_steps_value = cfg.get("blend_decay_steps")
    decay_steps = int(decay_steps_value) if decay_steps_value is not None else None

    return ModelBasedHybridWrapper(
        env,
        planner,
        initial_blend=float(cfg["initial_blend"]),
        min_blend=float(cfg["min_blend"]),
        blend_decay_steps=decay_steps,
    )


# -----------------------------------------------------------------------------
# Evaluation helpers
# -----------------------------------------------------------------------------


def evaluate_policy(
    env: gym.Env,
    planner: "ModelBasedPlanner",
    episodes: int,
    max_episode_steps: int,
) -> List[float]:
    rewards: List[float] = []
    for _ in range(episodes):
        obs, _ = env.reset()
        total_reward = 0.0
        for _ in range(max_episode_steps):
            action = planner.plan(obs)
            obs, reward, terminated, truncated, _ = env.step(action)
            total_reward += reward
            if terminated or truncated:
                break
        rewards.append(total_reward)
    return rewards


# -----------------------------------------------------------------------------
# Entrypoint
# -----------------------------------------------------------------------------


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Model-based RL demo for the lift task")
    parser.add_argument("--env-id", type=str, default="Isaac-Lift-Cube-Franka-v0", help="Gym environment id")
    parser.add_argument("--dataset-size", type=int, default=2000, help="Number of random transitions to collect")
    parser.add_argument("--max-episode-steps", type=int, default=200, help="Cutoff for episode length")
    parser.add_argument("--epochs", type=int, default=25, help="Number of training epochs for the dynamics model")
    parser.add_argument("--batch-size", type=int, default=128, help="Training batch size")
    parser.add_argument("--mpc-horizon", type=int, default=5, help="Planning horizon for MPC")
    parser.add_argument("--mpc-samples", type=int, default=256, help="Number of sampled action sequences")
    parser.add_argument("--headless", action="store_true", help="Run the simulator without rendering")
    parser.add_argument(
        "--show-usage",
        action="store_true",
        help="Print step-by-step instructions for running the demo and exit",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()

    if args.show_usage:
        print(
            "Model-based Lift demo usage:\n"
            "1. From the repository root run: python -m lift.model_based_lift --headless\n"
            "   (omit --headless to render).\n"
            "2. Adjust --dataset-size, --epochs, or MPC flags to change training/planning.\n"
            "3. The script collects data, trains the dynamics model, and evaluates it automatically.\n"
            "4. To launch the hybrid model-based/model-free demo, train with the\n"
            "   Gym id Isaac-Lift-Cube-Franka-Hybrid-v0.\n"
            "No other files need modification to try the demo."
        )
        return

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    env = gym.make(args.env_id, headless=args.headless)

    try:
        transitions = collect_transitions(
            env=env,
            num_samples=args.dataset_size,
            max_episode_steps=args.max_episode_steps,
        )

        dataset = TransitionDataset(transitions)
        obs_dim = dataset[0][0].shape[0]
        action_dim = dataset[0][1].shape[0]

        model = DynamicsModel(obs_dim=obs_dim, action_dim=action_dim).to(device)
        train_dynamics_model(model, dataset, args.batch_size, args.epochs, device)

        mpc_cfg = MPCConfig(horizon=args.mpc_horizon, num_action_sequences=args.mpc_samples)
        planner = ModelBasedPlanner(model=model, action_space=env.action_space, mpc_cfg=mpc_cfg, device=device)
        rewards = evaluate_policy(
            env=env,
            planner=planner,
            episodes=3,
            max_episode_steps=args.max_episode_steps,
        )

        avg_reward = float(np.mean(rewards))
        print(f"Average reward across {len(rewards)} evaluation episodes: {avg_reward:.3f}")
    finally:
        env.close()


if __name__ == "__main__":
    main()

